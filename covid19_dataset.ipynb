{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GcqQyghfo26F"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Embedding, Dense, Activation, MaxPool1D, Conv1D, LSTM\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test data for testing the model"
      ],
      "metadata": {
        "id": "6i5WD01z7PXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/Kaggle/twitterSentiment.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QD-3sIU355L",
        "outputId": "efef16da-a5fc-43dc-a080-8b9547d0eace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/Kaggle/twitterSentiment.zip\n",
            "  inflating: twitterSentiment.csv    \n",
            "  inflating: __MACOSX/._twitterSentiment.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# sklear\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, classifiimport pandas as pd \n",
        "\n",
        "# Importing the dataset\n",
        "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "dataset = pd.read_csv('/content/twitterSentiment.csv',\n",
        "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
        "\n",
        "# Removing the unnecessary columns.\n",
        "dataset = dataset[['sentiment','text']]\n",
        "# Replacing the values to ease understanding.\n",
        "dataset['sentiment'] = dataset['sentiment'].replace(4,1)\n",
        "\n",
        "# Plotting the distribution for dataset.\n",
        "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
        "\n",
        "# Storing data in lists.\n",
        "text, sentiment = list(dataset['text']), list(dataset['sentiment'])cation_report\n",
        "import pandas as pd \n",
        "\n",
        "# Importing the dataset\n",
        "DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "dataset = pd.read_csv('/content/twitterSentiment.csv',\n",
        "                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n",
        "\n",
        "# Removing the unnecessary columns.\n",
        "dataset = dataset[['sentiment','text']]\n",
        "# Replacing the values to ease understanding.\n",
        "dataset['sentiment'] = dataset['sentiment'].replace(4,1)\n",
        "\n",
        "# Plotting the distribution for dataset.\n",
        "ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
        "\n",
        "# Storing data in lists.\n",
        "text, sentiment = list(dataset['text']), list(dataset['sentiment'])\n",
        "\n",
        "print(text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "k1dzyJrv_6se",
        "outputId": "2d0d3571-2324-4b57-c550-9013a7fab2b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-8d452f5baafe>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    text, sentiment = list(dataset['text']), list(dataset['sentiment'])cation_report\u001b[0m\n\u001b[0m                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "  \n",
        "# Defining dictionary containing all emojis with their meanings.\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "## Defining set containing all stopwords in english.\n",
        "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
        "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
        "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
        "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
        "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
        "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
        "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
        "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
        "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
        "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
        "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
        "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
        "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
        "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
        "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "def preprocess(textdata):\n",
        "    processedText = []\n",
        "    \n",
        "    # Create Lemmatizer and Stemmer.\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "    \n",
        "    # Defining regex patterns.\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "    \n",
        "    for tweet in textdata:\n",
        "        tweet = tweet.lower()\n",
        "        \n",
        "        # Replace all URls with 'URL'\n",
        "        tweet = re.sub(urlPattern,' URL',tweet)\n",
        "        # Replace all emojis.\n",
        "        for emoji in emojis.keys():\n",
        "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n",
        "        # Replace @USERNAME to 'USER'.\n",
        "        tweet = re.sub(userPattern,' USER', tweet)        \n",
        "        # Replace all non alphabets.\n",
        "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "        # Replace 3 or more consecutive letters by 2 letter.\n",
        "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "        tweetwords = ''\n",
        "        for word in tweet.split():\n",
        "            # Checking if the word is a stopword.\n",
        "            #if word not in stopwordlist:\n",
        "            if len(word)>1:\n",
        "                # Lemmatizing the word.\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                tweetwords += (word+' ')\n",
        "            \n",
        "        processedText.append(tweetwords)\n",
        "        \n",
        "    return processedText\n",
        "\n",
        "import time\n",
        "t = time.time()\n",
        "processedtext = preprocess(text)\n",
        "\n",
        "print(f'Text Preprocessing complete.')\n",
        "print(f'Time Taken: {round(time.time()-t)} seconds')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
        "                                                    test_size = 0.3, random_state = 0)\n",
        "print(f'Data Split done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-EcyWSAFwF",
        "outputId": "44d98175-1c94-4776-f4b5-de99c53b8137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Preprocessing complete.\n",
            "Time Taken: 134 seconds\n",
            "Data Split done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpAVA2FEdS7K",
        "outputId": "1a8a19c3-1f4b-4baf-c7e4-69f6a571a1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it seems we are stuck on the ground in amarillo they have put ground stop for all flight leaving for denver said update in an hour \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Kaggle/IMDBDataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO3mpCbq2ROC",
        "outputId": "a45bcb57-fd4f-43b6-c196-43a31ce1169d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Kaggle/IMDBDataset.zip\n",
            "  inflating: IMDB Dataset.csv        \n",
            "  inflating: __MACOSX/._IMDB Dataset.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = \"/content/IMDB Dataset.csv\"\n",
        "\n",
        "df=pd.read_csv(dataset)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "akkDX7tW2g4I",
        "outputId": "5b250550-1411-42eb-a99a-1d1fd5614843"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43ae8827-a572-425a-a4b7-6ec0ace166ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43ae8827-a572-425a-a4b7-6ec0ace166ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43ae8827-a572-425a-a4b7-6ec0ace166ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43ae8827-a572-425a-a4b7-6ec0ace166ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "     \n",
        "\n",
        "import re\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('', '', text)\n",
        "     \n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "     \n",
        "df['review']=df['review'].apply(denoise_text)\n",
        "     \n",
        "df.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "dWxYm0P127JY",
        "outputId": "e5ddb7c6-8fc0-4e2e-acc9-5f3c066849a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               review sentiment\n",
              "0   One of the other reviewers has mentioned that ...  positive\n",
              "1   A wonderful little production. The filming tec...  positive\n",
              "2   I thought this was a wonderful way to spend ti...  positive\n",
              "3   Basically there's a family where a little boy ...  negative\n",
              "4   Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5   Probably my all-time favorite movie, a story o...  positive\n",
              "6   I sure would like to see a resurrection of a u...  positive\n",
              "7   This show was an amazing, fresh & innovative i...  negative\n",
              "8   Encouraged by the positive comments about this...  negative\n",
              "9   If you like original gut wrenching laughter yo...  positive\n",
              "10  Phil the Alien is one of those quirky films wh...  negative\n",
              "11  I saw this movie when I was about 12 when it c...  negative\n",
              "12  So im not a big fan of Boll's work but then ag...  negative\n",
              "13  The cast played Shakespeare.Shakespeare lost.I...  negative\n",
              "14  This a fantastic movie of three prisoners who ...  positive\n",
              "15  Kind of drawn in by the erotic scenes, only to...  negative\n",
              "16  Some films just simply should not be remade. T...  positive\n",
              "17  This movie made it into one of my top 10 most ...  negative\n",
              "18  I remember this film,it was the first film i h...  positive\n",
              "19  An awful film! It must have been up against so...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0252e926-ea7e-4558-96b0-a6c32e11a950\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>I saw this movie when I was about 12 when it c...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>So im not a big fan of Boll's work but then ag...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>The cast played Shakespeare.Shakespeare lost.I...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>This a fantastic movie of three prisoners who ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Kind of drawn in by the erotic scenes, only to...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Some films just simply should not be remade. T...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>This movie made it into one of my top 10 most ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>I remember this film,it was the first film i h...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>An awful film! It must have been up against so...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0252e926-ea7e-4558-96b0-a6c32e11a950')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0252e926-ea7e-4558-96b0-a6c32e11a950 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0252e926-ea7e-4558-96b0-a6c32e11a950');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "     \n",
        "def Convert_to_bin(text, remove_digits=True):\n",
        "    if text=='positive':\n",
        "      text= 1\n",
        "    else:\n",
        "      text=0\n",
        "    return text\n",
        "     \n",
        "\n",
        "df['review']=df['review'].apply(remove_special_characters)\n",
        "     \n",
        "\n",
        "df['sentiment']=df['sentiment'].apply(Convert_to_bin)\n",
        "     "
      ],
      "metadata": {
        "id": "Dn9c7Dci3T8I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(df['review'],df['sentiment'], test_size=0.3)\n"
      ],
      "metadata": {
        "id": "8U8CX6h23cGi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "1v1LVBt9KMq_",
        "outputId": "5034869b-7a5b-4a87-fb88-31f16aa7370d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-660a44740e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZrURePMxFJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Kaggle/covid2020dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMyS5e1Uxv4A",
        "outputId": "4fc04812-6315-4371-e1ef-3028b18706c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Kaggle/covid2020dataset.zip\n",
            "   creating: covid2020dataset/\n",
            "  inflating: __MACOSX/._covid2020dataset  \n",
            "  inflating: covid2020dataset/Covid-19 Twitter Dataset (Apr-Jun 2020).numbers  \n",
            "  inflating: __MACOSX/covid2020dataset/._Covid-19 Twitter Dataset (Apr-Jun 2020).numbers  \n",
            "  inflating: covid2020dataset/Covid-19 Twitter Dataset (Aug-Sep 2020).csv  \n",
            "  inflating: __MACOSX/covid2020dataset/._Covid-19 Twitter Dataset (Aug-Sep 2020).csv  \n",
            "  inflating: covid2020dataset/Covid-19 Twitter Dataset (Apr-Jun 2020).csv  \n",
            "  inflating: __MACOSX/covid2020dataset/._Covid-19 Twitter Dataset (Apr-Jun 2020).csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "     \n",
        "\n",
        "def sortMentions(file_path, column_name, mentions, sentiment_list):\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Use the csv.DictReader to read the file\n",
        "        reader = csv.DictReader(file)\n",
        "        # Extract the specified column and print its values\n",
        "        for row in reader:\n",
        "            for i in mentions:\n",
        "                if((row[column_name]) == i):\n",
        "                    sentiment_list.append([row['clean_tweet'], row['sentiment']])\n",
        "\n",
        "def duplicateCleanUp(file_path, cleaned_list, sentiment_list):\n",
        "      seen = set()\n",
        "      for item in sentiment_list:\n",
        "        t = tuple(item)\n",
        "        if t not in seen:\n",
        "          cleaned_list.append(item)\n",
        "          seen.add(t)\n",
        "          \n",
        "      #for i in sentiment_list:\n",
        "        #sentiment_list.count(i)\n",
        "      \n",
        "\n",
        "\n",
        "file_path = '/content/covid2020dataset/Covid-19 Twitter Dataset (Aug-Sep 2020).csv'\n",
        "column_name = 'user_mentions'\n",
        "#LISTS OF MENTIONS\n",
        "left_mentions = [\"HillaryClinton\", \"JoeBiden\", \"CNN\", \"SenKamalaHarris\", \"SenWarren\", \"SenGillibrand\", \"CoryBooker\", \"AlterNet\", \"AOC\", \"andrewcuomo\", \"FLOTUS\", \"donlemon\", \"MSNBC\", \"SpeakerPelosi\", \"jaketapper\", \"CuomoPrimeTime\"]\n",
        "right_mentions = [\"realDonladTrump\", \"TuckerCarlson\", \"FoxNews\", \"Mike_Pence\", \"SenTedCruz\", \"nypost\", \"theblaze\", \"RedState\", \"WalshFreedom\", \"benshapiro\", \"realDailyWire\", \"DonaldJTrumpJr\", \"GregAbbott_TX\", \"MailOnline\", \"WashTimes\", \"HawleyMO\", \"amconmag\", \"drudgereport\", \"megynkelly\", \"OANN\", \"TomiLahren\", \"Liz_Wheeler\", \"RealCandaceO\", \"RudyGiuliani\", \"BuckSexton\", \"IvankaTrump\", \"dbongino\", \"DennisPrager\", \"IngrahamAngle\", \"SarahHuckabee\", \"TheBabylonBee\"]\n",
        "#SORTED LISTS\n",
        "cleaned_left = []\n",
        "cleaned_right = []\n",
        "#SENTIMENT LISTS\n",
        "left_sentiment_list = []\n",
        "right_sentiment_list = []\n",
        "\n",
        "#left sided\n",
        "sortMentions(file_path, column_name, left_mentions, left_sentiment_list)\n",
        "#right sided\n",
        "sortMentions(file_path, column_name, right_mentions, right_sentiment_list)\n",
        "#sorted left sided\n",
        "duplicateCleanUp(file_path, cleaned_left, left_sentiment_list)\n",
        "#sorted right sided\n",
        "duplicateCleanUp(file_path, cleaned_right, right_sentiment_list)\n",
        "\n",
        "#text_left, sentiment_left = map(list, zip(*cleaned_left))\n",
        "\n",
        "\"\"\"\n",
        "temp = []\n",
        "def Convert_to_bin(sentiment):\n",
        "    for i in sentiment:\n",
        "      if i == 'pos':\n",
        "        temp.append(1)\n",
        "      elif i == 'neg':\n",
        "        temp.append(-1)\n",
        "      else:\n",
        "        temp.append(0)\n",
        "     \n",
        "Convert_to_bin(sentiment_left)\n",
        "sentiment_left = temp\n",
        "\n",
        "X_train = text_left[150:]\n",
        "X_test = text_left[:150]\n",
        "Y_train = sentiment_left[150:]\n",
        "Y_test = sentiment_left[:150]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "SNXX0UI3xKBa",
        "outputId": "09f57b2f-754e-4d0b-b22c-396eab57f20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ntemp = []\\ndef Convert_to_bin(sentiment):\\n    for i in sentiment:\\n      if i == 'pos':\\n        temp.append(1)\\n      elif i == 'neg':\\n        temp.append(-1)\\n      else:\\n        temp.append(0)\\n     \\nConvert_to_bin(sentiment_left)\\nsentiment_left = temp\\n\\nX_train = text_left[150:]\\nX_test = text_left[:150]\\nY_train = sentiment_left[150:]\\nY_test = sentiment_left[:150]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv \n",
        "header = ['text', 'sentiment']\n",
        "\n",
        "with open('covid2020.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # write the data\n",
        "    for row in cleaned_left:\n",
        "      writer.writerow(row)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataset = \"/content/covid2020.csv\"\n",
        "\n",
        "df=pd.read_csv(dataset)\n",
        "df=df.astype(str)\n",
        "\n",
        "df.head()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "     \n",
        "\n",
        "def Convert_to_bin(text, remove_digits=True):\n",
        "    if text=='positive':\n",
        "      text= 1\n",
        "    else:\n",
        "      text=0\n",
        "    return text\n",
        "     \n",
        "     \n",
        "\n",
        "df['sentiment']=df['sentiment'].apply(Convert_to_bin)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ7wnAFKIixh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test= train_test_split(df['text'],df['sentiment'], test_size=0.1)\n"
      ],
      "metadata": {
        "id": "KNMxYdkvb27b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train2, X_test2, Y_train2, Y_test2= train_test_split(df['text'],df['sentiment'], test_size=0.1)\n",
        "frames1 = [X_train, X_train2]\n",
        "frames2 = [X_test, X_test2]\n",
        "frames3 = [Y_train, Y_train2]\n",
        "frames4 = [Y_test, Y_test2]\n",
        "\n",
        "X_train = pd.concat(frames1)\n",
        "X_test = pd.concat(frames2)\n",
        "Y_train = pd.concat(frames3)\n",
        "Y_test = pd.concat(frames4)\n",
        "\n",
        "print(Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XloTxrOZdHC",
        "outputId": "ea843b41-464a-4b12-bf88-548420d48084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(918,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnQwonc3KJo5",
        "outputId": "bea18d63-4a08-42ef-ee63-4a3067b9ff02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "338    brazil suffer record daili coronaviru death th...\n",
              "189    need doctor reason relat covid19 telemedicin m...\n",
              "351    treatment effect anyon get icu soon covid19 te...\n",
              "12     studi surfac kind light type treatment differ ...\n",
              "462    cow scientist genet engin anim give immun syst...\n",
              "                             ...                        \n",
              "515    still asleep wheel case rise drastic peopl los...\n",
              "201    nichola gismondi hospit found older brother ra...\n",
              "45     california assemblywoman brought newborn daugh...\n",
              "395    presid trump disassoci realiti would funni dea...\n",
              "153    actual old crow joe death r directli tie covid...\n",
              "Name: text, Length: 102, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPdR_GiVRzow",
        "outputId": "eba51bec-6b71-4c4e-e7ee-0fce05763f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "112    bill author feder govt manufactur contract man...\n",
              "161    man austin texa arrest charg shove park ranger...\n",
              "170    question rep ratcliff confirm hear today next ...\n",
              "401    need covid19 test treatment vaccin abl distrib...\n",
              "95     maldivian aub india bjp gov make b covid19 pan...\n",
              "66     time give away money small group wealthi ameri...\n",
              "139    latest nbc news confirm covid19 case u report ...\n",
              "147    said hurrican maria say need accur death count...\n",
              "377    thought overthrow plan would occur coup would ...\n",
              "217    make mistak covid19 caus massiv econom challen...\n",
              "166    upward american die everi day covid19 number c...\n",
              "359    want share test posit covid19 viru seriou take...\n",
              "209    new studi find evid drug hydroxychloroquin hel...\n",
              "32     contact trace tool public health fieldwork id ...\n",
              "183    sit donald trump republican use covid19 excus ...\n",
              "432    third american survey cdc use kind riski clean...\n",
              "341    fail protect american covid19 presid trump ins...\n",
              "385    u soon reach death covid19 x number life lost ...\n",
              "28     presid trump suggest disinfect could elor poss...\n",
              "318    power cyclon form bay bengal head directli ind...\n",
              "309    former presid barack obama critic folk charg h...\n",
              "446                   problem riot would problem covid19\n",
              "456    gym chain hour fit file bankruptci perman clos...\n",
              "450    could bat hold secret covid19 meet scientist i...\n",
              "427    guy anoth traitor democraci enabl tri creat fa...\n",
              "75     covid19 continu awkwardli ee weird moral capit...\n",
              "258            covid19 respiratori diseas hit whole bodi\n",
              "428    scale econom devast caus covid19 stagger hous ...\n",
              "181    group young ineerienc volunt task secur much n...\n",
              "173    farmer alreadi dump milk destroy fresh produc ...\n",
              "40                                           abc covid19\n",
              "406    last month countless asian american threaten a...\n",
              "226    walmart respons keep worker safe covid19 fail ...\n",
              "327    must face ugli truth decad govt look way commu...\n",
              "390    mani peopl infect alreadi infect covid19 guess...\n",
              "255    anim shelter across u empti mani peopl rush ad...\n",
              "291    state number new covid19 case report day gener...\n",
              "130    german compani work u pharmaceut giant pfizer ...\n",
              "83     need know whether polit play role trump admini...\n",
              "278    doctor itali becom latest rais alarm worri spi...\n",
              "110    new data suggest patient sever covid19 took re...\n",
              "419    forc life forc natur robin greenfield famili r...\n",
              "142    sinc pompeo said claim support enorm evid pres...\n",
              "43     gon na blame trump brooklyn nurs home ravag co...\n",
              "215    trump care mar lago croni say advic public hea...\n",
              "153    top pediatrician detail possibl connect covid1...\n",
              "228    nebraska gov rickett announc state releas data...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Covid19 Sentiment Anlysis Model \n",
        "Try CNN model first than switch to RNN model\n",
        "\n",
        "First model will be using the count vectorizer method \n"
      ],
      "metadata": {
        "id": "Xwes9fBG4In_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "vec.fit(Y_train)\n",
        "x_train=vec.transform(Y_train)\n",
        "x_test=vec.transform(Y_test)\n",
        "\n",
        "print(x_train.shape[1])"
      ],
      "metadata": {
        "id": "_NbINhStcAeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d468d76-b6db-41b8-ab0b-5cf3228a79dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=x_train.shape[1], activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "nnbIHYN1v3_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gcQkZ-zrwaT",
        "outputId": "1937526f-4428-41fd-d12c-13ec2ae73f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                2825744   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,826,033\n",
            "Trainable params: 2,826,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd5HiJK_LGOI",
        "outputId": "8365ea3a-c462-41f0-c71e-fcd9331cf08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<35000x176608 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 4756998 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train = x_train.toarray() \n",
        "history = model.fit(x_train, Y_train,epochs=100,verbose=True,batch_size=16)"
      ],
      "metadata": {
        "id": "selP95WHngYe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bde553a-4619-42bf-aa4f-1bb306bf1307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-516d69f544f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x_train = x_train.toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nTypeError: 'SparseTensor' object is not subscriptable\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 268, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 146, in __call__\n    outputs = self._call(device, args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/script_ops.py\", line 153, in _call\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 477, in py_method\n    return [slice_array(inp) for inp in flat_inputs]\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 477, in <listcomp>\n    return [slice_array(inp) for inp in flat_inputs]\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\", line 475, in slice_array\n    return training_utils.slice_arrays(data, ind.numpy(),\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training_utils.py\", line 47, in slice_arrays\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training_utils.py\", line 47, in <listcomp>\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\n\n  File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training_utils.py\", line 47, in <listcomp>\n    entries = [[x[i:i + 1] for i in indices] for x in arrays]\n\nTypeError: 'SparseTensor' object is not subscriptable\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_759]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF model"
      ],
      "metadata": {
        "id": "ndbSgTJsB1fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vec = TfidfVectorizer()\n",
        "vec.fit(X_train)\n",
        "x_train=vec.transform(X_train)\n",
        "x_test=vec.transform(X_test)\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC(random_state=0)\n",
        "clf.fit(x_train,Y_train)\n",
        "y_test_pred=clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "report=classification_report(Y_test, y_test_pred,output_dict=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "9WoJvcV6B4wA",
        "outputId": "1f699461-7317-491c-f83b-095417114c4f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-21ef5ba3854a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Penalty term must be positive; got (C=%r)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [329, 141]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXwa-b8SDUtp",
        "outputId": "bb3f04e7-eabf-4fc7-cc25-c084fe376f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': {'precision': 0.9058186738836265,\n",
              "  'recall': 0.8890954974100146,\n",
              "  'f1-score': 0.8973791809102487,\n",
              "  'support': 7529},\n",
              " '1': {'precision': 0.8902759526938239,\n",
              "  'recall': 0.9068397804845402,\n",
              "  'f1-score': 0.8984815330548371,\n",
              "  'support': 7471},\n",
              " 'accuracy': 0.8979333333333334,\n",
              " 'macro avg': {'precision': 0.8980473132887252,\n",
              "  'recall': 0.8979676389472774,\n",
              "  'f1-score': 0.897930356982543,\n",
              "  'support': 15000},\n",
              " 'weighted avg': {'precision': 0.8980773625496922,\n",
              "  'recall': 0.8979333333333334,\n",
              "  'f1-score': 0.8979282257683967,\n",
              "  'support': 15000}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "H3fKC9E7Dzsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(X_train)\n",
        "x_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab = len(tokenizer.word_index) + 1 # num of unique words \n",
        "\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "maxlen = 150\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "lHtRkcl-D23n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim=50\n",
        "vocab=len(tokenizer.word_index)+1\n",
        "emb_mat= np.zeros((vocab,emb_dim))\n",
        "\n",
        "file_path=\"/content/drive/MyDrive/Kaggle/glove.6B.50d.txt\"\n",
        "\n",
        "with open(file_path) as f:\n",
        "        for line in f:\n",
        "            word, *emb = line.split()\n",
        "            if word in tokenizer.word_index:\n",
        "              ind=tokenizer.word_index[word]\n",
        "              emb_mat[ind]=np.array(emb,dtype=\"float32\")[:emb_dim]"
      ],
      "metadata": {
        "id": "vA4TmAX_d67M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epvk5m0SHqpu",
        "outputId": "9ce729e4-c41c-480c-e7e5-b7f979fc7b6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 36,  46,  33, ...,  24,  45, 245],\n",
              "       [ 10, 397,   5, ...,   0,   0,   0],\n",
              "       [224,  45,  48, ...,  20, 702,  62],\n",
              "       ...,\n",
              "       [  2,  27, 127, ..., 200, 104, 107],\n",
              "       [  9, 619,   3, ...,   0,   0,   0],\n",
              "       [233,  15,   8, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFp_d3tHChAB",
        "outputId": "a4a384d1-285d-4c10-dff1-81f794fa2756"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42026    1\n",
              "14091    0\n",
              "31472    0\n",
              "11949    0\n",
              "35847    0\n",
              "        ..\n",
              "37917    0\n",
              "11368    0\n",
              "16269    1\n",
              "5237     1\n",
              "29879    1\n",
              "Name: sentiment, Length: 35000, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 150 # match maxlen?\n",
        "model = Sequential([\n",
        "    Embedding(input_dim = vocab, output_dim=emb_dim, input_length=maxlen),\n",
        "    Conv1D(64, 5, activation='relu'),\n",
        "    MaxPool1D((5)), \n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPool1D((5)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jD8Q5bNpT0_L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYG64imwW7vP",
        "outputId": "7e719eb2-6628-4331-ce0e-61cc09f69366"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 150, 150)          26605500  \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 146, 64)           48064     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 29, 64)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 25, 128)           41088     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 5, 128)           0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5, 64)             8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5, 32)             2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,705,021\n",
            "Trainable params: 26,705,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_score = model.evaluate(x_train, Y_train)\n",
        "train_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PakcWD1ZXXIj",
        "outputId": "74bf36a4-d9b2-45aa-e8e7-26763b01fb06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1094/1094 [==============================] - 23s 20ms/step - loss: 0.6930 - accuracy: 0.5079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6930055022239685, 0.5079368948936462]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = model.evaluate(x_test, Y_test)\n",
        "test_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cjiGMITXvjc",
        "outputId": "f531882e-9148-4240-aea8-8e9b70831685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6932 - accuracy: 0.5001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6931668519973755, 0.5001081824302673]"
            ]
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, Y_train, epochs = 10, batch_size = 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "mLVaOHdsU4mX",
        "outputId": "2df594fe-23be-44e8-d3fe-73458b2765f4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  43/1094 [>.............................] - ETA: 6:44 - loss: 0.6931 - accuracy: 0.5161"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3b792bedc1bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m       (concrete_function,\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_test[:1]).flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwPexAZDQZaa",
        "outputId": "0f891dbf-06be-459a-b3fc-5450584238df"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.18445596, 0.37745154, 0.5529583 , 0.05679338, 0.5933913 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\", history)"
      ],
      "metadata": {
        "id": "zMB7S3l6fmML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vader Sentiment Model Analysis"
      ],
      "metadata": {
        "id": "S1u3VKQRffxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv  \n",
        "\n",
        "def sortMentions(file_path, column_name, mentions, sentiment_list):\n",
        "    with open(file_path, 'r') as file:\n",
        "        # Use the csv.DictReader to read the file\n",
        "        reader = csv.DictReader(file)\n",
        "        # Extract the specified column and print its values\n",
        "        for row in reader:\n",
        "            for i in mentions:\n",
        "                if((row[column_name]) == i):\n",
        "                    sentiment_list.append([row['clean_tweet'], row['sentiment']])\n",
        "\n",
        "def duplicateCleanUp(file_path, cleaned_list, sentiment_list):\n",
        "      seen = set()\n",
        "      for item in sentiment_list:\n",
        "        t = tuple(item)\n",
        "        if t not in seen:\n",
        "          cleaned_list.append(item)\n",
        "          seen.add(t)\n",
        "          \n",
        "      #for i in sentiment_list:\n",
        "        #sentiment_list.count(i)\n",
        "      \n",
        "\n",
        "\n",
        "file_path = '/content/covid2020dataset/Covid-19 Twitter Dataset (Apr-Jun 2020).csv'\n",
        "column_name = 'user_mentions'\n",
        "#LISTS OF MENTIONS\n",
        "left_mentions = [\"HillaryClinton\", \"JoeBiden\", \"CNN\", \"SenKamalaHarris\", \"SenWarren\", \"SenGillibrand\", \"CoryBooker\", \"AlterNet\", \"AOC\", \"andrewcuomo\", \"FLOTUS\", \"donlemon\", \"MSNBC\", \"SpeakerPelosi\", \"jaketapper\", \"CuomoPrimeTime\"]\n",
        "right_mentions = [\"realDonladTrump\", \"TuckerCarlson\", \"FoxNews\", \"Mike_Pence\", \"SenTedCruz\", \"nypost\", \"theblaze\", \"RedState\", \"WalshFreedom\", \"benshapiro\", \"realDailyWire\", \"DonaldJTrumpJr\", \"GregAbbott_TX\", \"MailOnline\", \"WashTimes\", \"HawleyMO\", \"amconmag\", \"drudgereport\", \"megynkelly\", \"OANN\", \"TomiLahren\", \"Liz_Wheeler\", \"RealCandaceO\", \"RudyGiuliani\", \"BuckSexton\", \"IvankaTrump\", \"dbongino\", \"DennisPrager\", \"IngrahamAngle\", \"SarahHuckabee\", \"TheBabylonBee\"]\n",
        "#SORTED LISTS\n",
        "cleaned_left1 = []\n",
        "cleaned_right1 = []\n",
        "#SENTIMENT LISTS\n",
        "left_sentiment_list1 = []\n",
        "right_sentiment_list1 = []\n",
        "\n",
        "#SORTED LISTS\n",
        "cleaned_left2 = []\n",
        "cleaned_right2 = []\n",
        "#SENTIMENT LISTS\n",
        "left_sentiment_list2 = []\n",
        "right_sentiment_list2 = []\n",
        "\n",
        "\n",
        "#left sided\n",
        "sortMentions(file_path, column_name, left_mentions, left_sentiment_list1)\n",
        "#right sided\n",
        "sortMentions(file_path, column_name, right_mentions, right_sentiment_list1)\n",
        "\n",
        "#sorter left sided\n",
        "duplicateCleanUp(file_path, cleaned_left1, left_sentiment_list1)\n",
        "#sorted right sided\n",
        "duplicateCleanUp(file_path, cleaned_right1, right_sentiment_list1)\n",
        "\n",
        "text_left1, sentiment_left1 = map(list, zip(*cleaned_left1))\n",
        "text_right1, sentiment_right1 = map(list, zip(*cleaned_right1))\n",
        "\n",
        "file_path = '/content/covid2020dataset/Covid-19 Twitter Dataset (Aug-Sep 2020).csv'\n",
        "\n",
        "#left sided\n",
        "sortMentions(file_path, column_name, left_mentions, left_sentiment_list2)\n",
        "#right sided\n",
        "sortMentions(file_path, column_name, right_mentions, right_sentiment_list2)\n",
        "#sorter left sided\n",
        "duplicateCleanUp(file_path, cleaned_left2, left_sentiment_list2)\n",
        "#sorted right sided\n",
        "duplicateCleanUp(file_path, cleaned_right2, right_sentiment_list2)\n",
        "\n",
        "text_left2, sentiment_left2 = map(list, zip(*cleaned_left2))\n",
        "text_right2, sentiment_right2 = map(list, zip(*cleaned_right2))\n",
        "\n",
        "text_left = text_left1 + text_left2\n",
        "text_right = text_right1 + text_right2\n",
        "\n",
        "sentiment_left = sentiment_left1 + sentiment_left2\n",
        "sentiment_right = sentiment_right1 + sentiment_right2\n",
        "\n",
        "# temp = []\n",
        "# def Convert_to_bin(sentiment):\n",
        "#     for i in sentiment:\n",
        "#       print(i)\n",
        "#       if i == 'pos':\n",
        "#         temp.append(1)\n",
        "#       elif i == 'neg':\n",
        "#         temp.append(-1)\n",
        "#       else:\n",
        "#         temp.append(0)\n",
        "     \n",
        "# Convert_to_bin(sentiment)\n",
        "# sentiment = temp\n",
        "\n",
        "# #left\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test= train_test_split(text,sentiment, test_size=0.3)\n",
        "\n",
        "#X_train1, X_test1, Y_train1, Y_test1= train_test_split(cleaned_left, left_sentiment_list, test_size=0.2)\n",
        "# X_train2, X_test2, Y_train2, Y_test2= train_test_split(cleaned_left2, left_sentiment_list2, test_size=0.2)\n",
        "\n",
        "#right\n",
        "# X_train1, X_test1, Y_train1, Y_test1= train_test_split(cleaned_right1, left_sentiment_right1, test_size=0.2)\n",
        "# X_train2, X_test2, Y_train2, Y_test2= train_test_split(cleaned_right2, left_sentiment_right2, test_size=0.2)\n",
        "\n"
      ],
      "metadata": {
        "id": "lcHrkJqmrIGX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrapper "
      ],
      "metadata": {
        "id": "uN82eoh8z-a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q snscrape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtU94u8Vz9xK",
        "outputId": "81b6aedf-6b14-4527-fb87-8c076a6dc4d4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#declare the list\n",
        "tweet_database = []\n",
        "tweet_database1D = []\n",
        "tweet_database_final_clean = []\n",
        "\n",
        "\n",
        "#set end date use date.today() for present\n",
        "today = '2016-06-06'\n",
        "end_date = today\n",
        "\n",
        "#keyword\n",
        "search_term = 'election'\n",
        "#beginning date\n",
        "from_date = '2015-01-01'\n",
        "\n",
        "max_results = 5000\n",
        "\n",
        "\n",
        "extracted_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-search '{search_term} until:{end_date}' > extracted-tweets.txt\"\n",
        "#tweet extractor\n",
        "os.system(extracted_tweets)\n",
        "#determines if no tweets were found\n",
        "if os.stat(\"extracted-tweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "#finds tweets with listed keyword content\n",
        "else:\n",
        "  df = pd.read_csv('extracted-tweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    tweet_database.append(row)\n",
        "\n",
        "#sets username\n",
        "user_name = \"USERNAME\"\n",
        "user_tweets = \"snscrape --format '{content!r}'\"+ f\" --max-results {max_results} --since {from_date} twitter-user '{user_name} until:{end_date}' > user-tweets.txt\"\n",
        "\n",
        "os.system(user_tweets)\n",
        "#determines if no tweets were found\n",
        "if os.stat(\"user-tweets.txt\").st_size == 0:\n",
        "  print('No Tweets found')\n",
        "#finds tweets from that user within givin set date\n",
        "else:\n",
        "  df = pd.read_csv('user-tweets.txt', names=['content'])\n",
        "  for row in df['content'].iteritems():\n",
        "    tweet_database.append(row)\n",
        "     \n",
        "tweet_database1D = np.array(tweet_database).T.tolist()\n",
        "\n",
        "temp = []\n",
        "for tweet in tweet_database1D[0]:\n",
        "  temp.append(tweet)\n",
        "\n",
        "tweet_database1D = temp\n",
        "\n",
        "#REMOVING URL Function\n",
        "def remove_urls (v_text):\n",
        "    v_text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', v_text, flags=re.MULTILINE)\n",
        "    return(v_text)\n",
        "\n",
        "#Duplicate remover changes tweet_database to cleaned_list\n",
        "cleaned_list = list(set(tweet_database1D))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdnbNHLx0uD2",
        "outputId": "04022488-1c9d-49c9-f7c4-4ab9a93cd4e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Tweets found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweet_database1D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qRdtFXe6hd2",
        "outputId": "f6097ebb-39f8-460d-d2df-a0a51a96e4f7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894', '895', '896', '897', '898', '899', '900', '901', '902', '903', '904', '905', '906', '907', '908', '909', '910', '911', '912', '913', '914', '915', '916', '917', '918', '919', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1029', '1030', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1038', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', '1074', '1075', '1076', '1077', '1078', '1079', '1080', '1081', '1082', '1083', '1084', '1085', '1086', '1087', '1088', '1089', '1090', '1091', '1092', '1093', '1094', '1095', '1096', '1097', '1098', '1099', '1100', '1101', '1102', '1103', '1104', '1105', '1106', '1107', '1108', '1109', '1110', '1111', '1112', '1113', '1114', '1115', '1116', '1117', '1118', '1119', '1120', '1121', '1122', '1123', '1124', '1125', '1126', '1127', '1128', '1129', '1130', '1131', '1132', '1133', '1134', '1135', '1136', '1137', '1138', '1139', '1140', '1141', '1142', '1143', '1144', '1145', '1146', '1147', '1148', '1149', '1150', '1151', '1152', '1153', '1154', '1155', '1156', '1157', '1158', '1159', '1160', '1161', '1162', '1163', '1164', '1165', '1166', '1167', '1168', '1169', '1170', '1171', '1172', '1173', '1174', '1175', '1176', '1177', '1178', '1179', '1180', '1181', '1182', '1183', '1184', '1185', '1186', '1187', '1188', '1189', '1190', '1191', '1192', '1193', '1194', '1195', '1196', '1197', '1198', '1199', '1200', '1201', '1202', '1203', '1204', '1205', '1206', '1207', '1208', '1209', '1210', '1211', '1212', '1213', '1214', '1215', '1216', '1217', '1218', '1219', '1220', '1221', '1222', '1223', '1224', '1225', '1226', '1227', '1228', '1229', '1230', '1231', '1232', '1233', '1234', '1235', '1236', '1237', '1238', '1239', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1248', '1249', '1250', '1251', '1252', '1253', '1254', '1255', '1256', '1257', '1258', '1259', '1260', '1261', '1262', '1263', '1264', '1265', '1266', '1267', '1268', '1269', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1277', '1278', '1279', '1280', '1281', '1282', '1283', '1284', '1285', '1286', '1287', '1288', '1289', '1290', '1291', '1292', '1293', '1294', '1295', '1296', '1297', '1298', '1299', '1300', '1301', '1302', '1303', '1304', '1305', '1306', '1307', '1308', '1309', '1310', '1311', '1312', '1313', '1314', '1315', '1316', '1317', '1318', '1319', '1320', '1321', '1322', '1323', '1324', '1325', '1326', '1327', '1328', '1329', '1330', '1331', '1332', '1333', '1334', '1335', '1336', '1337', '1338', '1339', '1340', '1341', '1342', '1343', '1344', '1345', '1346', '1347', '1348', '1349', '1350', '1351', '1352', '1353', '1354', '1355', '1356', '1357', '1358', '1359', '1360', '1361', '1362', '1363', '1364', '1365', '1366', '1367', '1368', '1369', '1370', '1371', '1372', '1373', '1374', '1375', '1376', '1377', '1378', '1379', '1380', '1381', '1382', '1383', '1384', '1385', '1386', '1387', '1388', '1389', '1390', '1391', '1392', '1393', '1394', '1395', '1396', '1397', '1398', '1399', '1400', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1409', '1410', '1411', '1412', '1413', '1414', '1415', '1416', '1417', '1418', '1419', '1420', '1421', '1422', '1423', '1424', '1425', '1426', '1427', '1428', '1429', '1430', '1431', '1432', '1433', '1434', '1435', '1436', '1437', '1438', '1439', '1440', '1441', '1442', '1443', '1444', '1445', '1446', '1447', '1448', '1449', '1450', '1451', '1452', '1453', '1454', '1455', '1456', '1457', '1458', '1459', '1460', '1461', '1462', '1463', '1464', '1465', '1466', '1467', '1468', '1469', '1470', '1471', '1472', '1473', '1474', '1475', '1476', '1477', '1478', '1479', '1480', '1481', '1482', '1483', '1484', '1485', '1486', '1487', '1488', '1489', '1490', '1491', '1492', '1493', '1494', '1495', '1496', '1497', '1498', '1499', '1500', '1501', '1502', '1503', '1504', '1505', '1506', '1507', '1508', '1509', '1510', '1511', '1512', '1513', '1514', '1515', '1516', '1517', '1518', '1519', '1520', '1521', '1522', '1523', '1524', '1525', '1526', '1527', '1528', '1529', '1530', '1531', '1532', '1533', '1534', '1535', '1536', '1537', '1538', '1539', '1540', '1541', '1542', '1543', '1544', '1545', '1546', '1547', '1548', '1549', '1550', '1551', '1552', '1553', '1554', '1555', '1556', '1557', '1558', '1559', '1560', '1561', '1562', '1563', '1564', '1565', '1566', '1567', '1568', '1569', '1570', '1571', '1572', '1573', '1574', '1575', '1576', '1577', '1578', '1579', '1580', '1581', '1582', '1583', '1584', '1585', '1586', '1587', '1588', '1589', '1590', '1591', '1592', '1593', '1594', '1595', '1596', '1597', '1598', '1599', '1600', '1601', '1602', '1603', '1604', '1605', '1606', '1607', '1608', '1609', '1610', '1611', '1612', '1613', '1614', '1615', '1616', '1617', '1618', '1619', '1620', '1621', '1622', '1623', '1624', '1625', '1626', '1627', '1628', '1629', '1630', '1631', '1632', '1633', '1634', '1635', '1636', '1637', '1638', '1639', '1640', '1641', '1642', '1643', '1644', '1645', '1646', '1647', '1648', '1649', '1650', '1651', '1652', '1653', '1654', '1655', '1656', '1657', '1658', '1659', '1660', '1661', '1662', '1663', '1664', '1665', '1666', '1667', '1668', '1669', '1670', '1671', '1672', '1673', '1674', '1675', '1676', '1677', '1678', '1679', '1680', '1681', '1682', '1683', '1684', '1685', '1686', '1687', '1688', '1689', '1690', '1691', '1692', '1693', '1694', '1695', '1696', '1697', '1698', '1699', '1700', '1701', '1702', '1703', '1704', '1705', '1706', '1707', '1708', '1709', '1710', '1711', '1712', '1713', '1714', '1715', '1716', '1717', '1718', '1719', '1720', '1721', '1722', '1723', '1724', '1725', '1726', '1727', '1728', '1729', '1730', '1731', '1732', '1733', '1734', '1735', '1736', '1737', '1738', '1739', '1740', '1741', '1742', '1743', '1744', '1745', '1746', '1747', '1748', '1749', '1750', '1751', '1752', '1753', '1754', '1755', '1756', '1757', '1758', '1759', '1760', '1761', '1762', '1763', '1764', '1765', '1766', '1767', '1768', '1769', '1770', '1771', '1772', '1773', '1774', '1775', '1776', '1777', '1778', '1779', '1780', '1781', '1782', '1783', '1784', '1785', '1786', '1787', '1788', '1789', '1790', '1791', '1792', '1793', '1794', '1795', '1796', '1797', '1798', '1799', '1800', '1801', '1802', '1803', '1804', '1805', '1806', '1807', '1808', '1809', '1810', '1811', '1812', '1813', '1814', '1815', '1816', '1817', '1818', '1819', '1820', '1821', '1822', '1823', '1824', '1825', '1826', '1827', '1828', '1829', '1830', '1831', '1832', '1833', '1834', '1835', '1836', '1837', '1838', '1839', '1840', '1841', '1842', '1843', '1844', '1845', '1846', '1847', '1848', '1849', '1850', '1851', '1852', '1853', '1854', '1855', '1856', '1857', '1858', '1859', '1860', '1861', '1862', '1863', '1864', '1865', '1866', '1867', '1868', '1869', '1870', '1871', '1872', '1873', '1874', '1875', '1876', '1877', '1878', '1879', '1880', '1881', '1882', '1883', '1884', '1885', '1886', '1887', '1888', '1889', '1890', '1891', '1892', '1893', '1894', '1895', '1896', '1897', '1898', '1899', '1900', '1901', '1902', '1903', '1904', '1905', '1906', '1907', '1908', '1909', '1910', '1911', '1912', '1913', '1914', '1915', '1916', '1917', '1918', '1919', '1920', '1921', '1922', '1923', '1924', '1925', '1926', '1927', '1928', '1929', '1930', '1931', '1932', '1933', '1934', '1935', '1936', '1937', '1938', '1939', '1940', '1941', '1942', '1943', '1944', '1945', '1946', '1947', '1948', '1949', '1950', '1951', '1952', '1953', '1954', '1955', '1956', '1957', '1958', '1959', '1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026', '2027', '2028', '2029', '2030', '2031', '2032', '2033', '2034', '2035', '2036', '2037', '2038', '2039', '2040', '2041', '2042', '2043', '2044', '2045', '2046', '2047', '2048', '2049', '2050', '2051', '2052', '2053', '2054', '2055', '2056', '2057', '2058', '2059', '2060', '2061', '2062', '2063', '2064', '2065', '2066', '2067', '2068', '2069', '2070', '2071', '2072', '2073', '2074', '2075', '2076', '2077', '2078', '2079', '2080', '2081', '2082', '2083', '2084', '2085', '2086', '2087', '2088', '2089', '2090', '2091', '2092', '2093', '2094', '2095', '2096', '2097', '2098', '2099', '2100', '2101', '2102', '2103', '2104', '2105', '2106', '2107', '2108', '2109', '2110', '2111', '2112', '2113', '2114', '2115', '2116', '2117', '2118', '2119', '2120', '2121', '2122', '2123', '2124', '2125', '2126', '2127', '2128', '2129', '2130', '2131', '2132', '2133', '2134', '2135', '2136', '2137', '2138', '2139', '2140', '2141', '2142', '2143', '2144', '2145', '2146', '2147', '2148', '2149', '2150', '2151', '2152', '2153', '2154', '2155', '2156', '2157', '2158', '2159', '2160', '2161', '2162', '2163', '2164', '2165', '2166', '2167', '2168', '2169', '2170', '2171', '2172', '2173', '2174', '2175', '2176', '2177', '2178', '2179', '2180', '2181', '2182', '2183', '2184', '2185', '2186', '2187', '2188', '2189', '2190', '2191', '2192', '2193', '2194', '2195', '2196', '2197', '2198', '2199', '2200', '2201', '2202', '2203', '2204', '2205', '2206', '2207', '2208', '2209', '2210', '2211', '2212', '2213', '2214', '2215', '2216', '2217', '2218', '2219', '2220', '2221', '2222', '2223', '2224', '2225', '2226', '2227', '2228', '2229', '2230', '2231', '2232', '2233', '2234', '2235', '2236', '2237', '2238', '2239', '2240', '2241', '2242', '2243', '2244', '2245', '2246', '2247', '2248', '2249', '2250', '2251', '2252', '2253', '2254', '2255', '2256', '2257', '2258', '2259', '2260', '2261', '2262', '2263', '2264', '2265', '2266', '2267', '2268', '2269', '2270', '2271', '2272', '2273', '2274', '2275', '2276', '2277', '2278', '2279', '2280', '2281', '2282', '2283', '2284', '2285', '2286', '2287', '2288', '2289', '2290', '2291', '2292', '2293', '2294', '2295', '2296', '2297', '2298', '2299', '2300', '2301', '2302', '2303', '2304', '2305', '2306', '2307', '2308', '2309', '2310', '2311', '2312', '2313', '2314', '2315', '2316', '2317', '2318', '2319', '2320', '2321', '2322', '2323', '2324', '2325', '2326', '2327', '2328', '2329', '2330', '2331', '2332', '2333', '2334', '2335', '2336', '2337', '2338', '2339', '2340', '2341', '2342', '2343', '2344', '2345', '2346', '2347', '2348', '2349', '2350', '2351', '2352', '2353', '2354', '2355', '2356', '2357', '2358', '2359', '2360', '2361', '2362', '2363', '2364', '2365', '2366', '2367', '2368', '2369', '2370', '2371', '2372', '2373', '2374', '2375', '2376', '2377', '2378', '2379', '2380', '2381', '2382', '2383', '2384', '2385', '2386', '2387', '2388', '2389', '2390', '2391', '2392', '2393', '2394', '2395', '2396', '2397', '2398', '2399', '2400', '2401', '2402', '2403', '2404', '2405', '2406', '2407', '2408', '2409', '2410', '2411', '2412', '2413', '2414', '2415', '2416', '2417', '2418', '2419', '2420', '2421', '2422', '2423', '2424', '2425', '2426', '2427', '2428', '2429', '2430', '2431', '2432', '2433', '2434', '2435', '2436', '2437', '2438', '2439', '2440', '2441', '2442', '2443', '2444', '2445', '2446', '2447', '2448', '2449', '2450', '2451', '2452', '2453', '2454', '2455', '2456', '2457', '2458', '2459', '2460', '2461', '2462', '2463', '2464', '2465', '2466', '2467', '2468', '2469', '2470', '2471', '2472', '2473', '2474', '2475', '2476', '2477', '2478', '2479', '2480', '2481', '2482', '2483', '2484', '2485', '2486', '2487', '2488', '2489', '2490', '2491', '2492', '2493', '2494', '2495', '2496', '2497', '2498', '2499', '2500', '2501', '2502', '2503', '2504', '2505', '2506', '2507', '2508', '2509', '2510', '2511', '2512', '2513', '2514', '2515', '2516', '2517', '2518', '2519', '2520', '2521', '2522', '2523', '2524', '2525', '2526', '2527', '2528', '2529', '2530', '2531', '2532', '2533', '2534', '2535', '2536', '2537', '2538', '2539', '2540', '2541', '2542', '2543', '2544', '2545', '2546', '2547', '2548', '2549', '2550', '2551', '2552', '2553', '2554', '2555', '2556', '2557', '2558', '2559', '2560', '2561', '2562', '2563', '2564', '2565', '2566', '2567', '2568', '2569', '2570', '2571', '2572', '2573', '2574', '2575', '2576', '2577', '2578', '2579', '2580', '2581', '2582', '2583', '2584', '2585', '2586', '2587', '2588', '2589', '2590', '2591', '2592', '2593', '2594', '2595', '2596', '2597', '2598', '2599', '2600', '2601', '2602', '2603', '2604', '2605', '2606', '2607', '2608', '2609', '2610', '2611', '2612', '2613', '2614', '2615', '2616', '2617', '2618', '2619', '2620', '2621', '2622', '2623', '2624', '2625', '2626', '2627', '2628', '2629', '2630', '2631', '2632', '2633', '2634', '2635', '2636', '2637', '2638', '2639', '2640', '2641', '2642', '2643', '2644', '2645', '2646', '2647', '2648', '2649', '2650', '2651', '2652', '2653', '2654', '2655', '2656', '2657', '2658', '2659', '2660', '2661', '2662', '2663', '2664', '2665', '2666', '2667', '2668', '2669', '2670', '2671', '2672', '2673', '2674', '2675', '2676', '2677', '2678', '2679', '2680', '2681', '2682', '2683', '2684', '2685', '2686', '2687', '2688', '2689', '2690', '2691', '2692', '2693', '2694', '2695', '2696', '2697', '2698', '2699', '2700', '2701', '2702', '2703', '2704', '2705', '2706', '2707', '2708', '2709', '2710', '2711', '2712', '2713', '2714', '2715', '2716', '2717', '2718', '2719', '2720', '2721', '2722', '2723', '2724', '2725', '2726', '2727', '2728', '2729', '2730', '2731', '2732', '2733', '2734', '2735', '2736', '2737', '2738', '2739', '2740', '2741', '2742', '2743', '2744', '2745', '2746', '2747', '2748', '2749', '2750', '2751', '2752', '2753', '2754', '2755', '2756', '2757', '2758', '2759', '2760', '2761', '2762', '2763', '2764', '2765', '2766', '2767', '2768', '2769', '2770', '2771', '2772', '2773', '2774', '2775', '2776', '2777', '2778', '2779', '2780', '2781', '2782', '2783', '2784', '2785', '2786', '2787', '2788', '2789', '2790', '2791', '2792', '2793', '2794', '2795', '2796', '2797', '2798', '2799', '2800', '2801', '2802', '2803', '2804', '2805', '2806', '2807', '2808', '2809', '2810', '2811', '2812', '2813', '2814', '2815', '2816', '2817', '2818', '2819', '2820', '2821', '2822', '2823', '2824', '2825', '2826', '2827', '2828', '2829', '2830', '2831', '2832', '2833', '2834', '2835', '2836', '2837', '2838', '2839', '2840', '2841', '2842', '2843', '2844', '2845', '2846', '2847', '2848', '2849', '2850', '2851', '2852', '2853', '2854', '2855', '2856', '2857', '2858', '2859', '2860', '2861', '2862', '2863', '2864', '2865', '2866', '2867', '2868', '2869', '2870', '2871', '2872', '2873', '2874', '2875', '2876', '2877', '2878', '2879', '2880', '2881', '2882', '2883', '2884', '2885', '2886', '2887', '2888', '2889', '2890', '2891', '2892', '2893', '2894', '2895', '2896', '2897', '2898', '2899', '2900', '2901', '2902', '2903', '2904', '2905', '2906', '2907', '2908', '2909', '2910', '2911', '2912', '2913', '2914', '2915', '2916', '2917', '2918', '2919', '2920', '2921', '2922', '2923', '2924', '2925', '2926', '2927', '2928', '2929', '2930', '2931', '2932', '2933', '2934', '2935', '2936', '2937', '2938', '2939', '2940', '2941', '2942', '2943', '2944', '2945', '2946', '2947', '2948', '2949', '2950', '2951', '2952', '2953', '2954', '2955', '2956', '2957', '2958', '2959', '2960', '2961', '2962', '2963', '2964', '2965', '2966', '2967', '2968', '2969', '2970', '2971', '2972', '2973', '2974', '2975', '2976', '2977', '2978', '2979', '2980', '2981', '2982', '2983', '2984', '2985', '2986', '2987', '2988', '2989', '2990', '2991', '2992', '2993', '2994', '2995', '2996', '2997', '2998', '2999', '3000', '3001', '3002', '3003', '3004', '3005', '3006', '3007', '3008', '3009', '3010', '3011', '3012', '3013', '3014', '3015', '3016', '3017', '3018', '3019', '3020', '3021', '3022', '3023', '3024', '3025', '3026', '3027', '3028', '3029', '3030', '3031', '3032', '3033', '3034', '3035', '3036', '3037', '3038', '3039', '3040', '3041', '3042', '3043', '3044', '3045', '3046', '3047', '3048', '3049', '3050', '3051', '3052', '3053', '3054', '3055', '3056', '3057', '3058', '3059', '3060', '3061', '3062', '3063', '3064', '3065', '3066', '3067', '3068', '3069', '3070', '3071', '3072', '3073', '3074', '3075', '3076', '3077', '3078', '3079', '3080', '3081', '3082', '3083', '3084', '3085', '3086', '3087', '3088', '3089', '3090', '3091', '3092', '3093', '3094', '3095', '3096', '3097', '3098', '3099', '3100', '3101', '3102', '3103', '3104', '3105', '3106', '3107', '3108', '3109', '3110', '3111', '3112', '3113', '3114', '3115', '3116', '3117', '3118', '3119', '3120', '3121', '3122', '3123', '3124', '3125', '3126', '3127', '3128', '3129', '3130', '3131', '3132', '3133', '3134', '3135', '3136', '3137', '3138', '3139', '3140', '3141', '3142', '3143', '3144', '3145', '3146', '3147', '3148', '3149', '3150', '3151', '3152', '3153', '3154', '3155', '3156', '3157', '3158', '3159', '3160', '3161', '3162', '3163', '3164', '3165', '3166', '3167', '3168', '3169', '3170', '3171', '3172', '3173', '3174', '3175', '3176', '3177', '3178', '3179', '3180', '3181', '3182', '3183', '3184', '3185', '3186', '3187', '3188', '3189', '3190', '3191', '3192', '3193', '3194', '3195', '3196', '3197', '3198', '3199', '3200', '3201', '3202', '3203', '3204', '3205', '3206', '3207', '3208', '3209', '3210', '3211', '3212', '3213', '3214', '3215', '3216', '3217', '3218', '3219', '3220', '3221', '3222', '3223', '3224', '3225', '3226', '3227', '3228', '3229', '3230', '3231', '3232', '3233', '3234', '3235', '3236', '3237', '3238', '3239', '3240', '3241', '3242', '3243', '3244', '3245', '3246', '3247', '3248', '3249', '3250', '3251', '3252', '3253', '3254', '3255', '3256', '3257', '3258', '3259', '3260', '3261', '3262', '3263', '3264', '3265', '3266', '3267', '3268', '3269', '3270', '3271', '3272', '3273', '3274', '3275', '3276', '3277', '3278', '3279', '3280', '3281', '3282', '3283', '3284', '3285', '3286', '3287', '3288', '3289', '3290', '3291', '3292', '3293', '3294', '3295', '3296', '3297', '3298', '3299', '3300', '3301', '3302', '3303', '3304', '3305', '3306', '3307', '3308', '3309', '3310', '3311', '3312', '3313', '3314', '3315', '3316', '3317', '3318', '3319', '3320', '3321', '3322', '3323', '3324', '3325', '3326', '3327', '3328', '3329', '3330', '3331', '3332', '3333', '3334', '3335', '3336', '3337', '3338', '3339', '3340', '3341', '3342', '3343', '3344', '3345', '3346', '3347', '3348', '3349', '3350', '3351', '3352', '3353', '3354', '3355', '3356', '3357', '3358', '3359', '3360', '3361', '3362', '3363', '3364', '3365', '3366', '3367', '3368', '3369', '3370', '3371', '3372', '3373', '3374', '3375', '3376', '3377', '3378', '3379', '3380', '3381', '3382', '3383', '3384', '3385', '3386', '3387', '3388', '3389', '3390', '3391', '3392', '3393', '3394', '3395', '3396', '3397', '3398', '3399', '3400', '3401', '3402', '3403', '3404', '3405', '3406', '3407', '3408', '3409', '3410', '3411', '3412', '3413', '3414', '3415', '3416', '3417', '3418', '3419', '3420', '3421', '3422', '3423', '3424', '3425', '3426', '3427', '3428', '3429', '3430', '3431', '3432', '3433', '3434', '3435', '3436', '3437', '3438', '3439', '3440', '3441', '3442', '3443', '3444', '3445', '3446', '3447', '3448', '3449', '3450', '3451', '3452', '3453', '3454', '3455', '3456', '3457', '3458', '3459', '3460', '3461', '3462', '3463', '3464', '3465', '3466', '3467', '3468', '3469', '3470', '3471', '3472', '3473', '3474', '3475', '3476', '3477', '3478', '3479', '3480', '3481', '3482', '3483', '3484', '3485', '3486', '3487', '3488', '3489', '3490', '3491', '3492', '3493', '3494', '3495', '3496', '3497', '3498', '3499', '3500', '3501', '3502', '3503', '3504', '3505', '3506', '3507', '3508', '3509', '3510', '3511', '3512', '3513', '3514', '3515', '3516', '3517', '3518', '3519', '3520', '3521', '3522', '3523', '3524', '3525', '3526', '3527', '3528', '3529', '3530', '3531', '3532', '3533', '3534', '3535', '3536', '3537', '3538', '3539', '3540', '3541', '3542', '3543', '3544', '3545', '3546', '3547', '3548', '3549', '3550', '3551', '3552', '3553', '3554', '3555', '3556', '3557', '3558', '3559', '3560', '3561', '3562', '3563', '3564', '3565', '3566', '3567', '3568', '3569', '3570', '3571', '3572', '3573', '3574', '3575', '3576', '3577', '3578', '3579', '3580', '3581', '3582', '3583', '3584', '3585', '3586', '3587', '3588', '3589', '3590', '3591', '3592', '3593', '3594', '3595', '3596', '3597', '3598', '3599', '3600', '3601', '3602', '3603', '3604', '3605', '3606', '3607', '3608', '3609', '3610', '3611', '3612', '3613', '3614', '3615', '3616', '3617', '3618', '3619', '3620', '3621', '3622', '3623', '3624', '3625', '3626', '3627', '3628', '3629', '3630', '3631', '3632', '3633', '3634', '3635', '3636', '3637', '3638', '3639', '3640', '3641', '3642', '3643', '3644', '3645', '3646', '3647', '3648', '3649', '3650', '3651', '3652', '3653', '3654', '3655', '3656', '3657', '3658', '3659', '3660', '3661', '3662', '3663', '3664', '3665', '3666', '3667', '3668', '3669', '3670', '3671', '3672', '3673', '3674', '3675', '3676', '3677', '3678', '3679', '3680', '3681', '3682', '3683', '3684', '3685', '3686', '3687', '3688', '3689', '3690', '3691', '3692', '3693', '3694', '3695', '3696', '3697', '3698', '3699', '3700', '3701', '3702', '3703', '3704', '3705', '3706', '3707', '3708', '3709', '3710', '3711', '3712', '3713', '3714', '3715', '3716', '3717', '3718', '3719', '3720', '3721', '3722', '3723', '3724', '3725', '3726', '3727', '3728', '3729', '3730', '3731', '3732', '3733', '3734', '3735', '3736', '3737', '3738', '3739', '3740', '3741', '3742', '3743', '3744', '3745', '3746', '3747', '3748', '3749', '3750', '3751', '3752', '3753', '3754', '3755', '3756', '3757', '3758', '3759', '3760', '3761', '3762', '3763', '3764', '3765', '3766', '3767', '3768', '3769', '3770', '3771', '3772', '3773', '3774', '3775', '3776', '3777', '3778', '3779', '3780', '3781', '3782', '3783', '3784', '3785', '3786', '3787', '3788', '3789', '3790', '3791', '3792', '3793', '3794', '3795', '3796', '3797', '3798', '3799', '3800', '3801', '3802', '3803', '3804', '3805', '3806', '3807', '3808', '3809', '3810', '3811', '3812', '3813', '3814', '3815', '3816', '3817', '3818', '3819', '3820', '3821', '3822', '3823', '3824', '3825', '3826', '3827', '3828', '3829', '3830', '3831', '3832', '3833', '3834', '3835', '3836', '3837', '3838', '3839', '3840', '3841', '3842', '3843', '3844', '3845', '3846', '3847', '3848', '3849', '3850', '3851', '3852', '3853', '3854', '3855', '3856', '3857', '3858', '3859', '3860', '3861', '3862', '3863', '3864', '3865', '3866', '3867', '3868', '3869', '3870', '3871', '3872', '3873', '3874', '3875', '3876', '3877', '3878', '3879', '3880', '3881', '3882', '3883', '3884', '3885', '3886', '3887', '3888', '3889', '3890', '3891', '3892', '3893', '3894', '3895', '3896', '3897', '3898', '3899', '3900', '3901', '3902', '3903', '3904', '3905', '3906', '3907', '3908', '3909', '3910', '3911', '3912', '3913', '3914', '3915', '3916', '3917', '3918', '3919', '3920', '3921', '3922', '3923', '3924', '3925', '3926', '3927', '3928', '3929', '3930', '3931', '3932', '3933', '3934', '3935', '3936', '3937', '3938', '3939', '3940', '3941', '3942', '3943', '3944', '3945', '3946', '3947', '3948', '3949', '3950', '3951', '3952', '3953', '3954', '3955', '3956', '3957', '3958', '3959', '3960', '3961', '3962', '3963', '3964', '3965', '3966', '3967', '3968', '3969', '3970', '3971', '3972', '3973', '3974', '3975', '3976', '3977', '3978', '3979', '3980', '3981', '3982', '3983', '3984', '3985', '3986', '3987', '3988', '3989', '3990', '3991', '3992', '3993', '3994', '3995', '3996', '3997', '3998', '3999', '4000', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '4080', '4081', '4082', '4083', '4084', '4085', '4086', '4087', '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '4100', '4101', '4102', '4103', '4104', '4105', '4106', '4107', '4108', '4109', '4110', '4111', '4112', '4113', '4114', '4115', '4116', '4117', '4118', '4119', '4120', '4121', '4122', '4123', '4124', '4125', '4126', '4127', '4128', '4129', '4130', '4131', '4132', '4133', '4134', '4135', '4136', '4137', '4138', '4139', '4140', '4141', '4142', '4143', '4144', '4145', '4146', '4147', '4148', '4149', '4150', '4151', '4152', '4153', '4154', '4155', '4156', '4157', '4158', '4159', '4160', '4161', '4162', '4163', '4164', '4165', '4166', '4167', '4168', '4169', '4170', '4171', '4172', '4173', '4174', '4175', '4176', '4177', '4178', '4179', '4180', '4181', '4182', '4183', '4184', '4185', '4186', '4187', '4188', '4189', '4190', '4191', '4192', '4193', '4194', '4195', '4196', '4197', '4198', '4199', '4200', '4201', '4202', '4203', '4204', '4205', '4206', '4207', '4208', '4209', '4210', '4211', '4212', '4213', '4214', '4215', '4216', '4217', '4218', '4219', '4220', '4221', '4222', '4223', '4224', '4225', '4226', '4227', '4228', '4229', '4230', '4231', '4232', '4233', '4234', '4235', '4236', '4237', '4238', '4239', '4240', '4241', '4242', '4243', '4244', '4245', '4246', '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254', '4255', '4256', '4257', '4258', '4259', '4260', '4261', '4262', '4263', '4264', '4265', '4266', '4267', '4268', '4269', '4270', '4271', '4272', '4273', '4274', '4275', '4276', '4277', '4278', '4279', '4280', '4281', '4282', '4283', '4284', '4285', '4286', '4287', '4288', '4289', '4290', '4291', '4292', '4293', '4294', '4295', '4296', '4297', '4298', '4299', '4300', '4301', '4302', '4303', '4304', '4305', '4306', '4307', '4308', '4309', '4310', '4311', '4312', '4313', '4314', '4315', '4316', '4317', '4318', '4319', '4320', '4321', '4322', '4323', '4324', '4325', '4326', '4327', '4328', '4329', '4330', '4331', '4332', '4333', '4334', '4335', '4336', '4337', '4338', '4339', '4340', '4341', '4342', '4343', '4344', '4345', '4346', '4347', '4348', '4349', '4350', '4351', '4352', '4353', '4354', '4355', '4356', '4357', '4358', '4359', '4360', '4361', '4362', '4363', '4364', '4365', '4366', '4367', '4368', '4369', '4370', '4371', '4372', '4373', '4374', '4375', '4376', '4377', '4378', '4379', '4380', '4381', '4382', '4383', '4384', '4385', '4386', '4387', '4388', '4389', '4390', '4391', '4392', '4393', '4394', '4395', '4396', '4397', '4398', '4399', '4400', '4401', '4402', '4403', '4404', '4405', '4406', '4407', '4408', '4409', '4410', '4411', '4412', '4413', '4414', '4415', '4416', '4417', '4418', '4419', '4420', '4421', '4422', '4423', '4424', '4425', '4426', '4427', '4428', '4429', '4430', '4431', '4432', '4433', '4434', '4435', '4436', '4437', '4438', '4439', '4440', '4441', '4442', '4443', '4444', '4445', '4446', '4447', '4448', '4449', '4450', '4451', '4452', '4453', '4454', '4455', '4456', '4457', '4458', '4459', '4460', '4461', '4462', '4463', '4464', '4465', '4466', '4467', '4468', '4469', '4470', '4471', '4472', '4473', '4474', '4475', '4476', '4477', '4478', '4479', '4480', '4481', '4482', '4483', '4484', '4485', '4486', '4487', '4488', '4489', '4490', '4491', '4492', '4493', '4494', '4495', '4496', '4497', '4498', '4499', '4500', '4501', '4502', '4503', '4504', '4505', '4506', '4507', '4508', '4509', '4510', '4511', '4512', '4513', '4514', '4515', '4516', '4517', '4518', '4519', '4520', '4521', '4522', '4523', '4524', '4525', '4526', '4527', '4528', '4529', '4530', '4531', '4532', '4533', '4534', '4535', '4536', '4537', '4538', '4539', '4540', '4541', '4542', '4543', '4544', '4545', '4546', '4547', '4548', '4549', '4550', '4551', '4552', '4553', '4554', '4555', '4556', '4557', '4558', '4559', '4560', '4561', '4562', '4563', '4564', '4565', '4566', '4567', '4568', '4569', '4570', '4571', '4572', '4573', '4574', '4575', '4576', '4577', '4578', '4579', '4580', '4581', '4582', '4583', '4584', '4585', '4586', '4587', '4588', '4589', '4590', '4591', '4592', '4593', '4594', '4595', '4596', '4597', '4598', '4599', '4600', '4601', '4602', '4603', '4604', '4605', '4606', '4607', '4608', '4609', '4610', '4611', '4612', '4613', '4614', '4615', '4616', '4617', '4618', '4619', '4620', '4621', '4622', '4623', '4624', '4625', '4626', '4627', '4628', '4629', '4630', '4631', '4632', '4633', '4634', '4635', '4636', '4637', '4638', '4639', '4640', '4641', '4642', '4643', '4644', '4645', '4646', '4647', '4648', '4649', '4650', '4651', '4652', '4653', '4654', '4655', '4656', '4657', '4658', '4659', '4660', '4661', '4662', '4663', '4664', '4665', '4666', '4667', '4668', '4669', '4670', '4671', '4672', '4673', '4674', '4675', '4676', '4677', '4678', '4679', '4680', '4681', '4682', '4683', '4684', '4685', '4686', '4687', '4688', '4689', '4690', '4691', '4692', '4693', '4694', '4695', '4696', '4697', '4698', '4699', '4700', '4701', '4702', '4703', '4704', '4705', '4706', '4707', '4708', '4709', '4710', '4711', '4712', '4713', '4714', '4715', '4716', '4717', '4718', '4719', '4720', '4721', '4722', '4723', '4724', '4725', '4726', '4727', '4728', '4729', '4730', '4731', '4732', '4733', '4734', '4735', '4736', '4737', '4738', '4739', '4740', '4741', '4742', '4743', '4744', '4745', '4746', '4747', '4748', '4749', '4750', '4751', '4752', '4753', '4754', '4755', '4756', '4757', '4758', '4759', '4760', '4761', '4762', '4763', '4764', '4765', '4766', '4767', '4768', '4769', '4770', '4771', '4772', '4773', '4774', '4775', '4776', '4777', '4778', '4779', '4780', '4781', '4782', '4783', '4784', '4785', '4786', '4787', '4788', '4789', '4790', '4791', '4792', '4793', '4794', '4795', '4796', '4797', '4798', '4799', '4800', '4801', '4802', '4803', '4804', '4805', '4806', '4807', '4808', '4809', '4810', '4811', '4812', '4813', '4814', '4815', '4816', '4817', '4818', '4819', '4820', '4821', '4822', '4823', '4824', '4825', '4826', '4827', '4828', '4829', '4830', '4831', '4832', '4833', '4834', '4835', '4836', '4837', '4838', '4839', '4840', '4841', '4842', '4843', '4844', '4845', '4846', '4847', '4848', '4849', '4850', '4851', '4852', '4853', '4854', '4855', '4856', '4857', '4858', '4859', '4860', '4861', '4862', '4863', '4864', '4865', '4866', '4867', '4868', '4869', '4870', '4871', '4872', '4873', '4874', '4875', '4876', '4877', '4878', '4879', '4880', '4881', '4882', '4883', '4884', '4885', '4886', '4887', '4888', '4889', '4890', '4891', '4892', '4893', '4894', '4895', '4896', '4897', '4898', '4899', '4900', '4901', '4902', '4903', '4904', '4905', '4906', '4907', '4908', '4909', '4910', '4911', '4912', '4913', '4914', '4915', '4916', '4917', '4918', '4919', '4920', '4921', '4922', '4923', '4924', '4925', '4926', '4927', '4928', '4929', '4930', '4931', '4932', '4933', '4934', '4935', '4936', '4937', '4938', '4939', '4940', '4941', '4942', '4943', '4944', '4945', '4946', '4947', '4948', '4949', '4950', '4951', '4952', '4953', '4954', '4955', '4956', '4957', '4958', '4959', '4960', '4961', '4962', '4963', '4964', '4965', '4966', '4967', '4968', '4969', '4970', '4971', '4972', '4973', '4974', '4975', '4976', '4977', '4978', '4979', '4980', '4981', '4982', '4983', '4984', '4985', '4986']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF8Ekbh-ozdC",
        "outputId": "56fce3c2-2448-49b8-b18e-b9b483367c6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from vaderSentiment) (2.25.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->vaderSentiment) (4.0.0)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sentiment_model = SentimentIntensityAnalyzer()\n",
        "sentiment_dict = sentiment_model.polarity_scores(\"My Condolences to the Family of those who did not survive #Covid_19!\")\n",
        "     \n",
        "print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
        "print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
        "print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
        "print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhzoaKCPfZSZ",
        "outputId": "e92c99c3-88f9-4a6f-efac-0219b5586051"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall sentiment dictionary is :  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "sentence was rated as  0.0 % Negative\n",
            "sentence was rated as  100.0 % Neutral\n",
            "sentence was rated as  0.0 % Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_right \n",
        "def show_data(text):\n",
        "  total_neg = 0\n",
        "  neg_count = 0\n",
        "  total_pos = 0\n",
        "  pos_count = 0\n",
        "  total_neu = 0\n",
        "  total_count = 0\n",
        "\n",
        "  for tweet in text:\n",
        "    total_count += 1\n",
        "    sentiment_dict = sentiment_model.polarity_scores(tweet)\n",
        "    sentiment_val = max(sentiment_dict['neg'], sentiment_dict['neu'],sentiment_dict['pos']) # gets the highest percentage \n",
        "    if(sentiment_dict['neg'] == sentiment_val):\n",
        "      total_neg+=1\n",
        "      neg_count+=1\n",
        "    elif sentiment_dict['pos'] == sentiment_val:\n",
        "      total_pos+=1\n",
        "      pos_count+=1\n",
        "    else:\n",
        "      total_neu+=1\n",
        "    \n",
        "  avg_neg = total_neg/total_count\n",
        "  avg_pos = total_pos/total_count\n",
        "  avg_neu = total_neu/total_count\n",
        "  print(\"Average Negative: \", avg_neg)\n",
        "  print(\"Average Positive: \", avg_pos)\n",
        "  print(\"Avergae Neutral: \", avg_neu)\n",
        "\n",
        "print(\"Right leaning politics\")\n",
        "show_data(text)\n",
        "text = text_left\n",
        "\n",
        "print(\"Left learning politics\")\n",
        "show_data(text)"
      ],
      "metadata": {
        "id": "AibHKdH0p2kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076dee15-1e8f-4c47-f5d8-651623c6db46"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Right leaning politics\n",
            "Average Negative:  0.05622489959839357\n",
            "Average Positive:  0.01606425702811245\n",
            "Avergae Neutral:  0.927710843373494\n",
            "Left learning politics\n",
            "Average Negative:  0.03333333333333333\n",
            "Average Positive:  0.011764705882352941\n",
            "Avergae Neutral:  0.9549019607843138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entire Covid Dataset"
      ],
      "metadata": {
        "id": "2A122p_SrC2C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beQTI_uq4zNx",
        "outputId": "890b17e6-06fe-4303-9a68-c6a6376dbe33"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['trump said last month anyon want test covid19 get one true still nearli', 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames1 = [X_train1, X_train2]\n",
        "frames2 = [X_test1, X_test2]\n",
        "frames3 = [Y_train1, Y_train2]\n",
        "frames4 = [Y_test1, Y_test2]\n",
        "\n",
        "X_train = pd.concat(frames1)\n",
        "X_test = pd.concat(frames2)\n",
        "Y_train = pd.concat(frames3)\n",
        "Y_test = pd.concat(frames4)"
      ],
      "metadata": {
        "id": "YQhcsQSPrW8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding"
      ],
      "metadata": {
        "id": "XlNh3SFhw4Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorflow_text>=2.0.0rc0"
      ],
      "metadata": {
        "id": "zvvB7pBaxZd5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")"
      ],
      "metadata": {
        "id": "7Qtaw36Jw6m3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Each sentence passed to the model is encoded as a vector with 512 elements"
      ],
      "metadata": {
        "id": "23eSwEFyxFqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv \n",
        "header = ['text', 'sentiment']\n",
        "\n",
        "with open('covid2020.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # write the data\n",
        "    for row in cleaned_left:\n",
        "      writer.writerow(row)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataset = \"/content/covid2020.csv\"\n",
        "\n",
        "df=pd.read_csv(dataset)\n",
        "df=df.astype(str)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lIVKsS8AzZ-p",
        "outputId": "d788bbef-de94-46f0-ec4d-4fbd4cf72876"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text sentiment\n",
              "0  trump said last month anyon want test covid19 ...       pos\n",
              "1  trump use elector map make life death decis di...       neg\n",
              "2  eert say need greatli eand covid19 test capac ...       pos\n",
              "3  peopl around globe point led govern effort inv...       pos\n",
              "4  limit le impact posit nation institut health b...       neu"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-def09aec-4b5a-4169-a042-07418b9502c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>trump said last month anyon want test covid19 ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>trump use elector map make life death decis di...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eert say need greatli eand covid19 test capac ...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>peopl around globe point led govern effort inv...</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>limit le impact posit nation institut health b...</td>\n",
              "      <td>neu</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-def09aec-4b5a-4169-a042-07418b9502c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-def09aec-4b5a-4169-a042-07418b9502c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-def09aec-4b5a-4169-a042-07418b9502c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "\n",
        "# Plotting the distribution for dataset.\n",
        "ax = df.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax.set_xticklabels(['Negative','Positive','Neutral'], rotation=0)\n",
        "\n",
        "# Storing data in lists.\n",
        "text, sentiment = list(dataset['text']), list(dataset['sentiment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "OlqkeZuCzbsP",
        "outputId": "7b8a2503-23c6-4232-c12e-207237e9de9e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1655fc35b50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Storing data in lists.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPwyPlfF1dRf",
        "outputId": "15b84d32-32dd-40fb-c999-970df19dc58b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 0,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvISgoOp2BOd",
        "outputId": "b1bf48ad-5ebc-496a-91a1-93c91fa385c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['trump said last month anyon want test covid19 get one true still nearli']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "type_one_hot = OneHotEncoder(sparse=False).fit_transform(\n",
        "  Y_train.reshape(-1, 1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdPknrJ949Ij",
        "outputId": "5f03278b-a593-4b1d-8d67-0d076c19ad3e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['hey joe scienc wrong tweeti',\n",
              "       'tri understand disinfect stori happen wh brief thursday bill bryan',\n",
              "       'hertz file bankruptci latest victim econom downturn amid covid19 compani rent car sinc',\n",
              "       'scale econom devast caus covid19 stagger hous democrat pass ppp flexibl act pr',\n",
              "       'coronaviru pandem broad power governor exercis emerg declar underscor limit',\n",
              "       'join senat kamala harri symon sander town hall address impact dispar covid19 black',\n",
              "       'nose swab finger prick differ test give u differ insight covid19 latest podcast coronaviru fact v',\n",
              "       'make mistak covid19 caus massiv econom challeng crisi hit u harder last longer becau',\n",
              "       'someon pleas ask dt would use disinfect suggest famili member use prevent cure covid19',\n",
              "       'covid19 affect anyon anywher impact everi commun equal watch dr eboni jade hilton elain',\n",
              "       'let link everyth covid19',\n",
              "       'worker face safeti violat work retali whistleblow may hard time file claim covid19',\n",
              "       'everi day think essenti worker work tirelessli get u',\n",
              "       'well go covid19 medium hysteriain recent week guess protest organ riot goi',\n",
              "       '',\n",
              "       'doubt form kawasaki connect covid19 cdc physician britain believ multisystem inf',\n",
              "       'former presid barack obama critic folk charg handl coronaviru pandem wake privat',\n",
              "       'privileg look like month push doj releas certain low risk vulner peopl',\n",
              "       'andrea circl bear first woman die feder prison covid19 pregnant babi surviv',\n",
              "       'cri jake piec tweet cuomo murphi condemn thousand senior',\n",
              "       'root caus covid19 epidem begin china govern gave fund wuhan',\n",
              "       'document trump campaign want sign case contract covid19 trump ralli http',\n",
              "       'serious ill covid19 patient treat hydroxychloroquin chloroquin like die develop dangero',\n",
              "       'worker infect peopl covid19 fish process plant ghana countri presid nana akufo addo say th',\n",
              "       'ask dr fauci number covid19 infect death unit state improv three month',\n",
              "       'biden covid19 mask peopl everi state wear',\n",
              "       'everi step way presid trump ignor eert downplay threat covid19 pose misl american',\n",
              "       'memo circul suggest survivor covid19 ban join militari sr pentagon',\n",
              "       'feder govern fail respons covid19 indian countri unaccept read new piec',\n",
              "       'trump like modern day jim jone invit will follow drink kool aid invit',\n",
              "       'racist attack asian asian american broader aapi commun covid19 pandem si',\n",
              "       'suprem court make histori hold argument phone covid19 sudden di',\n",
              "       'presid trump want use covid19 anoth excus roll back regul keep u safe big',\n",
              "       'thank memori victim covid19 prayer particularli impass one b',\n",
              "       'coronaviru pandem decim nyc polic fire depart first respond union citi offici',\n",
              "       'lead colleagu push save nativ tribal health facil middl pandem',\n",
              "       'day work covid19 unit arizona hospit icu nurs lauren leander threw pair clean scrub',\n",
              "       'covid19 guidelin restrict visit hospit care center john see father person',\n",
              "       'million case covid19 almost dead upset presid trump tough question press cri',\n",
              "       'peopl fought die right vote peopl liter put life risk cast ballot',\n",
              "       'presid promis honor lost covid19',\n",
              "       'could cost american govern trillion dollar within five year usag mask alon',\n",
              "       'democrat psychopath murder minor womb democrat origin supremacist r',\n",
              "       'hi speaker pelosi quest diagnost provid covid19 immun test cost link',\n",
              "       'twitter said thursday shut account tie chines govern eert work twitt',\n",
              "       'trump fire u top vaccin scientist call baloney presid unproven miracl cure covid19 sho',\n",
              "       'professor verg make signific find research covid19 shot kill appar murder suic',\n",
              "       'administr want believ reason see increas number case increas te',\n",
              "       'peopl u die covid19 sunday may noon et cnn hold memori',\n",
              "       'presid trump campaign ralli tulsa oklahoma saturday violat virtual everi one guid principl gather',\n",
              "       'februari month ago announc bill de fund trump racist border wall use fund help',\n",
              "       'everi turn presid trump ignor eert downplay threat covid19 pose misl american peopl',\n",
              "       'last hour th american die covid19 trump watch spent last hour share',\n",
              "       'group charg preserv french languag say covid19 feminin despit increasingli common usag masc',\n",
              "       'detroit nurs mom share emot video shift treat covid19 patient shock probabl one best way',\n",
              "       'hard truth bad lack prepar slow respons test failur',\n",
              "       'state feder govern need act urgenc make sure peopl effect lose right',\n",
              "       'leftist mayor governor murder thousand covid19 medium drone want worri',\n",
              "       'need covid19 test treatment vaccin abl distribut scale without swab vial',\n",
              "       'presid trump tri rewrit histori come china covid19 fact back toni blin',\n",
              "       'brazil suffer record daili coronaviru death thursday fast approach russia becom world',\n",
              "       'spaniard soak sun weekend figur releas countri ministri health sunday reveal new',\n",
              "       'china given point point rebutt preposter alleg lie said fabric u polit',\n",
              "       'u food drug administr yet approv drug treatment coronaviru plan announc',\n",
              "       'project air bridg jare kushner secret public privat partnership covid19 suppli fail state',\n",
              "       'gator get laid thank lot covid19 pretend like reason',\n",
              "       'cdc director total misquot fake news covid19 presid said twitter putt',\n",
              "       'sen kamala harri berni sander ed markey introduc bill aim establish rebat payment program would',\n",
              "       'data alon enough feder govern respons direct inform resourc commun',\n",
              "       'u respons covid19 consist inconsist also uniqu american symptom american individua',\n",
              "       'state number new covid19 case report day gener go state number holdin',\n",
              "       'new data suggest patient sever covid19 took remdesivir could recov faster patient take nat',\n",
              "       'german compani work u pharmaceut giant pfizer begun human trial potenti covid19 vaccin could sup',\n",
              "       'know test place get one along sever countri',\n",
              "       'use freedom religion argument demand cart blanch demand open religi venu proffer falla',\n",
              "       'covid19 disproportion impact nativ commun mani health center area ri',\n",
              "       'word presid matter donald trump use downplay covid19 pa blame onto other mislead',\n",
              "       'doctor pari hospit say found evid one patient admit decemb infect covid19 suggestin',\n",
              "       'group young ineerienc volunt task secur much need medic suppli hospit fight covid19',\n",
              "       'last thing need covid19 anoth war militari solut north korea nuclear progra',\n",
              "       'new asymptomat covid19 case found wuhan today accord citi health offici',\n",
              "       'covid19 pandem threaten undo year progress made toward global gender equiti let',\n",
              "       'hous democrat lead voic fight progress nationwid commit fight injustic wherev',\n",
              "       'pledg match ad inventori brand make new invest digit platform use fo',\n",
              "       'contact trace good way stop covid19 spread white hous everywher els congress incl',\n",
              "       'demand take vote next covid19 packag unless ke',\n",
              "       'almost half number soldier lost world war presidenti historian michael beschloss say',\n",
              "       'guy anoth traitor democraci enabl tri creat facsist regim rememb knew',\n",
              "       'report trump staffer ralli test posit covid19 shock',\n",
              "       'insist drug provid hospit patient confirm covid19 supervi',\n",
              "       'even gen zer digit nativ assumpt comfort live fulli digit life wrong eliza p shap',\n",
              "       'rep adam schiff say seen evid support claim covid19 origin chines lab',\n",
              "       'serious ill covid19 patient treat hydroxychloroquin chloroquin like die develop danger',\n",
              "       'firefight alreadi put life line everi day keep u safe equip need prot',\n",
              "       'amazon fire least worker rais concern covid19 safeti amazon warehous look like clear',\n",
              "       'ever america reason grate dreamer allow either heartless interpret care',\n",
              "       'dr anthoni fauci nation top infecti diseas eert said world health organ correct suggest',\n",
              "       'due ongo covid19 pandem necessari social distanc guidelin domest violenc victim even vul',\n",
              "       'white hous ignor threat covid19 late stop left state without equip fight',\n",
              "       'instead vote addit covid19 relief american desper need mitch mcconnel prioriti sen',\n",
              "       'eect covid19 either huh',\n",
              "       'frontlin crisi strike today fair pay basic protect',\n",
              "       'treatment effect anyon get icu soon covid19 test return posit',\n",
              "       'hit campaign trail person due covid19 instead travel across countri home delaw',\n",
              "       'someth think may get lost coverag new ihm u wash project covid19 death u',\n",
              "       'like fireman run burn build save anoth life regard anyth',\n",
              "       'covid19 respiratori diseas hit whole bodi',\n",
              "       'one coronaviru patient diabet die within first seven day hospit one five need ventila',\n",
              "       'world popul die use k number john',\n",
              "       'problem riot would problem covid19', 'time mr covid19 mr death',\n",
              "       'upward american die everi day covid19 number could rise day june c',\n",
              "       'state includ georgia arkansa california alabama number new covid19 case rise http',\n",
              "       'million refuge displac peopl face doubl emerg covid19 reach commun',\n",
              "       'least five u team clone antibodi covid19 pave way cut edg treatment could one resea',\n",
              "       'costco largest u retail enact rule aim help prevent spread covid19 compani say th',\n",
              "       'covid19 still kill hundr american day strangl economi meanwhil trump administr',\n",
              "       'put poll place groceri store walmart everyon stay safe',\n",
              "       'melania trump immigr lawyer slam presid embarrass respons covid19 pandem',\n",
              "       'enforc strict earli lockdown greec manag keep death incred low around far prime mini',\n",
              "       'world press freedom day journalist face onslaught repress form new law intimid autocra',\n",
              "       'spread covid19 unit state pose seriou health challeng also caus',\n",
              "       'actual worri peopl dr esther choo said respons presid trump suggest disinfec',\n",
              "       'presid trump sign law roughli billion packag deliv aid small busi hospit eand co',\n",
              "       'u suprem court resum oral argument monday follow hiatu due covid19 pandem biggest case involv acc',\n",
              "       'mani peopl infect alreadi infect covid19 guess care oh well l',\n",
              "       'behind everi covid19 death famili commun never behind everi unemploy claim li',\n",
              "       'cdc data suggest u significantli undercount death covid19 even worst flu year',\n",
              "       'like minor own busi deni ppp loan black own latino own nativ',\n",
              "       'u doctor say may seen possibl complic coronaviru infect young child rare inflammatori condit',\n",
              "       'snl close th season alec baldwin return role presid trump congratul class covid19',\n",
              "       'new studi found unless reduc number detain peopl ice detent center immigran',\n",
              "       'repeatedli ask state dept dod elain u militari hardwar sold saudi uae end arm',\n",
              "       'u may cope well right eert worri emot resili begin fray threat covid19',\n",
              "       'sen tammi duckworth say concern dept veteran affair respons coronaviru pandem',\n",
              "       'also said trump approach covid19 like laurel hardi push piano stair f',\n",
              "       'sir peopl usa fight togeth overcom soon pandem covid19 want see next presid usa',\n",
              "       'doctor warn today inflamm caus sever effect covid19 diseas patient said reduc eff',\n",
              "       'learn learn wrong say gov cuomo discuss process new inform coro',\n",
              "       'american never probabl go even hope guy guy like',\n",
              "       'cnn poll show peopl say u govern poor job prevent spread covid19 trump approv rate'],\n",
              "      dtype='<U97')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(X_test)\n",
        "\n",
        "\n",
        "import keras\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(\n",
        "  keras.layers.Dense(\n",
        "    units=256,\n",
        "    input_shape=(X_train.shape[1], ),\n",
        "    activation='relu'\n",
        "  )\n",
        ")\n",
        "model.add(\n",
        "  keras.layers.Dropout(rate=0.5)\n",
        ")\n",
        "\n",
        "model.add(\n",
        "  keras.layers.Dense(\n",
        "    units=128,\n",
        "    activation='relu'\n",
        "  )\n",
        ")\n",
        "model.add(\n",
        "  keras.layers.Dropout(rate=0.5)\n",
        ")\n",
        "\n",
        "model.add(keras.layers.Dense(2, activation='softmax'))\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=keras.optimizers.Adam(0.001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "OiyMJL5-5uDt",
        "outputId": "a12b971d-f86e-478b-d8ea-e9a828d97230"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-bd46bef576d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m   keras.layers.Dense(\n\u001b[1;32m     10\u001b[0m     \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   )\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r43eE-IR6l97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Do\n",
        "- Decide on what model to use \n",
        "- Most likely use the entire covid dataset\n",
        "- Use the model that achieved 89\n",
        "\n",
        "- Train on an older dataset and compare the averaged scores of negativity and positivity between 2020 and another \n",
        "\n",
        "- Finish flask \n",
        "- Display the graph\n",
        "- Display the \n",
        "\n",
        "-----------------\n",
        "- We have a general model for sentiment analysis\n",
        "- Train this model with covid dataset (left and right) and calcualte the average positive and negative for each side\n",
        "- Do this again with an older dataset and compare "
      ],
      "metadata": {
        "id": "ohaSNx3o-JJ8"
      }
    }
  ]
}